import streamlit as st
import pandas as pd
import os
import time
from typing import List

# LangChain & Gemini Imports
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv

# 1. Configuration & API Setup
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, ".env")

# 2. í•´ë‹¹ ê²½ë¡œì˜ .env íŒŒì¼ì„ ê°•ì œë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv(dotenv_path=env_path)

# (ë””ë²„ê¹…ìš©) í™”ë©´ì— ê²½ë¡œê°€ ì œëŒ€ë¡œ ì¡íˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. í•´ê²°ë˜ë©´ ì§€ìš°ì…”ë„ ë©ë‹ˆë‹¤.
st.write(f"ê²€ìƒ‰ ê²½ë¡œ: {env_path}")
st.write(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(env_path)}")

# 3. API í‚¤ í™•ì¸
if not os.getenv("GOOGLE_API_KEY"):
    st.error(f"API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {env_path}")
    st.stop()

GEMINI_MODEL = "gemini-2.0-flash"
EMBEDDING_MODEL = "models/embedding-001"

st.set_page_config(page_title="AnyData Chatbot", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ë‚´ íŒŒì¼ê³¼ ëŒ€í™”í•˜ê¸° (AnyData Chatbot)")

# 2. File Upload Logic
# =========================================================
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["csv", "xlsx"])

@st.cache_resource(show_spinner="AIê°€ ë¬¸ì„œë¥¼ ì½ê³  ìˆìŠµë‹ˆë‹¤... (ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
def process_uploaded_file(file):
    if file is None:
        return None, None

    # 1. íŒŒì¼ ì½ê¸°
    try:
        if file.name.endswith('.csv'):
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except UnicodeDecodeError:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp949')
        else:
            df = pd.read_excel(file)
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None

    # 2. í…ìŠ¤íŠ¸ ë³€í™˜ (Document ìƒì„±)
    documents = []
    for idx, row in df.iterrows():
        content_parts = []
        for col in df.columns:
            val = row[col]
            if pd.notna(val) and str(val).strip() != "":
                content_parts.append(f"{col}: {val}")
        
        page_content = "\n".join(content_parts)
        title_col = df.columns[0]
        row_title = str(row[title_col])[:50] 

        doc = Document(
            page_content=page_content,
            metadata={
                "row": idx,
                "source": file.name,
                "summary_title": row_title
            }
        )
        documents.append(doc)

    # 3. ì²­í¬ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = splitter.split_documents(documents)

    # 4. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ + ì†ë„ ì¡°ì ˆ)
    embedding = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    
    # ì§„í–‰ë¥  í‘œì‹œë°” ìƒì„±
    progress_text = "ë²¡í„° ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."
    my_bar = st.progress(0, text=progress_text)
    
    batch_size = 20  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ë„ˆë¬´ í¬ë©´ 429 ì—ëŸ¬ ë°œìƒ)
    total_splits = len(splits)
    vectorstore = None
    
    for i in range(0, total_splits, batch_size):
        batch = splits[i : i + batch_size]
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ VectorStore ìƒì„±, ê·¸ ì´í›„ëŠ” ì¶”ê°€(add)
        if vectorstore is None:
            vectorstore = FAISS.from_documents(batch, embedding=embedding)
        else:
            vectorstore.add_documents(batch)
            
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        percent_complete = min((i + batch_size) / total_splits, 1.0)
        my_bar.progress(percent_complete, text=f"ë²¡í„° ë³€í™˜ ì¤‘... ({int(percent_complete*100)}%)")
        
        # API ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ 1ì´ˆ ëŒ€ê¸° (ë°ì´í„°ê°€ ë§ìœ¼ë©´ 2~3ì´ˆë¡œ ëŠ˜ë¦¬ì„¸ìš”)
        time.sleep(1)

    my_bar.empty() # ì™„ë£Œë˜ë©´ ì§„í–‰ë°” ì‚­ì œ
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    return df, retriever

# 3. Main Logic
# =========================================================

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”! CSVë‚˜ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì‹œë©´ ë‚´ìš©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš”.")
    ]

# íŒŒì¼ ì²˜ë¦¬
if uploaded_file:
    df, retriever = process_uploaded_file(uploaded_file)
    if retriever:
        st.success(f"âœ… '{uploaded_file.name}' ë¶„ì„ ì™„ë£Œ! ({len(df)}ê°œì˜ ë°ì´í„°)")
else:
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œí•˜ê³  ì¤‘ë‹¨
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    df, retriever = None, None

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for msg in st.session_state["messages"]:
    st.chat_message(msg.role).write(msg.content)

# LLM ì´ˆê¸°í™”
llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì´ ë°ì´í„°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”")

if user_input and retriever:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))

    # ê²€ìƒ‰ (RAG)
    retrieved_docs = retriever.invoke(user_input)
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # í”„ë¡¬í”„íŠ¸ (ë²”ìš©)
    prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ [ë°ì´í„° ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        
        ê·œì¹™:
        1. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
        2. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        3. ì¶œì²˜(ë°ì´í„°ì˜ ë‚´ìš©)ë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        [ë°ì´í„° ë¬¸ë§¥]:
        {context}

        ì§ˆë¬¸:
        {question}

        ë‹µë³€:"""
    )

    chain = prompt | llm

    # ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
    with st.chat_message("assistant"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
            response_container = st.empty()
            full_response = ""
            
            for chunk in chain.stream({
                "question": user_input,
                "context": context_text
            }):
                full_response += chunk.content
                response_container.markdown(full_response)
            
            # ì¶œì²˜ í‘œì‹œ (ì„ íƒ ì‚¬í•­)
            source_titles = set([doc.metadata['summary_title'] for doc in retrieved_docs])
            if source_titles:
                st.caption(f"ì°¸ê³  ë°ì´í„°: {', '.join(list(source_titles)[:3])} ë“±")

            st.session_state["messages"].append(ChatMessage(role="assistant", content=full_response))

elif user_input and not retriever:
    st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")