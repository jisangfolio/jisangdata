
'''
demo quesitons
'''

import streamlit as st
import pandas as pd
import os
import time
from typing import List

# LangChain & Gemini Imports
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv

# 1. Configuration & API Setup
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, ".env")

load_dotenv(dotenv_path=env_path)

if not os.getenv("GOOGLE_API_KEY"):
    st.error("API KEY ERROR")
    st.stop()

GEMINI_MODEL = "gemini-2.0-flash"
EMBEDDING_MODEL = "models/embedding-001"

st.set_page_config(page_title="AnyData", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ë‚´ íŒŒì¼ê³¼ ëŒ€í™”í•˜ê¸° (AnyData)")

# 2. File Upload Logic
# =========================================================
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("CSV ë˜ëŠ” Excel íŒŒì¼ ì—…ë¡œë“œ", type=["csv", "xlsx"])

@st.cache_resource(show_spinner="ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¶„ì„ ì¤‘...")
def process_uploaded_file(file):
    if file is None:
        return None, None

    # 1. íŒŒì¼ ì½ê¸°
    try:
        if file.name.endswith('.csv'):
            try:
                df = pd.read_csv(file, encoding='utf-8')
            except UnicodeDecodeError:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp949')
        else:
            df = pd.read_excel(file)
    except Exception as e:
        st.error(f"âŒ íŒŒì¼ì„ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None

    # 2. í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ëª¨ë“  ì»¬ëŸ¼ í•©ì¹˜ê¸°)
    documents = []
    for idx, row in df.iterrows():
        content_parts = []
        for col in df.columns:
            val = row[col]
            if pd.notna(val) and str(val).strip() != "":
                content_parts.append(f"{col}: {val}")

        page_content = "\n".join(content_parts)

        title_col = df.columns[0]
        row_title = str(row[title_col])[:50] 
        
        # ë¬¸ì„œ ê°ì²´ ìƒì„±
        doc = Document(
            page_content=page_content,
            metadata={
                "row": idx,
                "source": file.name,
                "summary_title": row_title
            }
        )
        documents.append(doc)

    # 3. ì²­í¬ ë¶„í•  (Split)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = splitter.split_documents(documents)

    # 4. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
    embedding = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
    
    # ì§„í–‰ë¥  í‘œì‹œì¤„
    progress_bar = st.progress(0, text="ë°ì´í„° ì €ì¥ ì‹œì‘...")
    
    vectorstore = None
    batch_size = 20
    total_splits = len(splits)

    for i in range(0, total_splits, batch_size):
        batch = splits[i : i + batch_size]
        
        if vectorstore is None:
            vectorstore = FAISS.from_documents(batch, embedding=embedding)
        else:
            vectorstore.add_documents(batch)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        percent = min((i + batch_size) / total_splits, 1.0)
        progress_bar.progress(percent, text=f"ë°ì´í„° ì €ì¥ ì¤‘... ({int(percent*100)}%)")
        
        time.sleep(1)

    progress_bar.empty()

    if vectorstore:
        st.sidebar.success(f"âœ… ìµœì¢… ì €ì¥ëœ ë°ì´í„° ìˆ˜: {vectorstore.index.ntotal}ê°œ")
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 50})
    return df, retriever

# 3. Main Logic
# =========================================================

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="CSVë‚˜ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì‹œë©´ ë‚´ìš©ì„ ë¶„ì„í•´ ë“œë¦´ê²Œìš”.")
    ]

# íŒŒì¼ ì²˜ë¦¬
if uploaded_file:
    df, retriever = process_uploaded_file(uploaded_file)
    if retriever:
        st.success(f"âœ… '{uploaded_file.name}' ë¶„ì„ ì™„ë£Œ! ({len(df)}ê°œì˜ ë°ì´í„°)")
else:
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œí•˜ê³  ì¤‘ë‹¨
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    df, retriever = None, None

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for msg in st.session_state["messages"]:
    st.chat_message(msg.role).write(msg.content)

# LLM ì´ˆê¸°í™”
llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì´ ë°ì´í„°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”")
 
if user_input and retriever:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))

    # ê²€ìƒ‰ (RAG)
    retrieved_docs = retriever.invoke(user_input)
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # í”„ë¡¬í”„íŠ¸ (ë²”ìš©)
    prompt = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ [ë°ì´í„° ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        
        ê·œì¹™:
        1. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
        2. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        3. ì¶œì²˜(ë°ì´í„°ì˜ ë‚´ìš©)ë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        [ë°ì´í„° ë¬¸ë§¥]:
        {context}

        ì§ˆë¬¸:
        {question}

        ë‹µë³€:"""
    )

    chain = prompt | llm

    # ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
    with st.chat_message("assistant"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
            response_container = st.empty()
            full_response = ""
            
            for chunk in chain.stream({
                "question": user_input,
                "context": context_text
            }):
                full_response += chunk.content
                response_container.markdown(full_response)
            
            # ì¶œì²˜ í‘œì‹œ (ì„ íƒ ì‚¬í•­)
            # source_titles = set([doc.metadata['summary_title'] for doc in retrieved_docs])
            # if source_titles:
            #     st.caption(f"ì°¸ê³  ë°ì´í„°: {', '.join(list(source_titles)[:3])} ë“±")

            st.session_state["messages"].append(ChatMessage(role="assistant", content=full_response))

elif user_input and not retriever:
    st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")